{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka9-BtIZhpgT"
      },
      "source": [
        "# Ungraded Lab: Denoising with a CNN Autoencoder\n",
        "\n",
        "In the final lab for this week, you will introduce noise to the Fashion MNIST dataset and train an autoencoder to reconstruct the original input images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-WXDV3Tk6N6"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3EXwoz-KHtWO"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#   # %tensorflow_version only exists in Colab.\n",
        "#   %tensorflow_version 2.x\n",
        "# except Exception:\n",
        "#   pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX4GbMpwk8Y5"
      },
      "source": [
        "## Prepare the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLuUvE7ieuAl"
      },
      "source": [
        "You will prepare the train and test sets a little differently this time. Instead of just normalizing the images, you will also introduce random noise and the generated images will be used as input to your model. The target or label will still be the clean images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t9F7YsCNIKSA"
      },
      "outputs": [],
      "source": [
        "def map_image_with_noise(image, label):\n",
        "  '''Normalizes the images and generates noisy inputs.'''\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = image / 255.0\n",
        "\n",
        "  noise_factor = 0.5\n",
        "  factor = noise_factor * tf.random.normal(shape=image.shape)\n",
        "  image_noisy = image + factor\n",
        "  image_noisy = tf.clip_by_value(image_noisy, 0.0, 1.0)\n",
        "\n",
        "  return image_noisy, image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ub3k-XfMeTol"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "SHUFFLE_BUFFER_SIZE = 1024\n",
        "\n",
        "train_dataset = tfds.load('fashion_mnist', as_supervised=True, split=\"train\")\n",
        "train_dataset = train_dataset.map(map_image_with_noise)\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "test_dataset = tfds.load('fashion_mnist', as_supervised=True, split=\"test\")\n",
        "test_dataset = test_dataset.map(map_image_with_noise)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dJ4QWDMk_Wd"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8t_TQP3fOLZ"
      },
      "source": [
        "You will use the same model from the previous lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_8SD8jRfVG7"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=15zh7bst9KKvciRdCvMAH7kXt3nNkABzO\" width=\"75%\" height=\"75%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wxh8h-UMk2iL"
      },
      "outputs": [],
      "source": [
        "def encoder(inputs):\n",
        "  '''Defines the encoder with two Conv2D and max pooling layers.'''\n",
        "  conv_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "  max_pool_1 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv_1)\n",
        "\n",
        "  conv_2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(max_pool_1)\n",
        "  max_pool_2 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv_2)\n",
        "\n",
        "  return max_pool_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wRWmLA3VliDr"
      },
      "outputs": [],
      "source": [
        "def bottle_neck(inputs):\n",
        "  '''Defines the bottleneck.'''\n",
        "  bottle_neck = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "  encoder_visualization = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(bottle_neck)\n",
        "\n",
        "  return bottle_neck, encoder_visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XZgLt5uAmArk"
      },
      "outputs": [],
      "source": [
        "def decoder(inputs):\n",
        "  '''Defines the decoder path to upsample back to the original image size.'''\n",
        "  conv_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "  up_sample_1 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_1)\n",
        "\n",
        "  conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(up_sample_1)\n",
        "  up_sample_2 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_2)\n",
        "\n",
        "  conv_3 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(up_sample_2)\n",
        "\n",
        "  return conv_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fQKwO64iiOYl"
      },
      "outputs": [],
      "source": [
        "def convolutional_auto_encoder():\n",
        "  '''Builds the entire autoencoder model.'''\n",
        "  inputs = tf.keras.layers.Input(shape=(28, 28, 1,))\n",
        "  encoder_output = encoder(inputs)\n",
        "  bottleneck_output, encoder_visualization = bottle_neck(encoder_output)\n",
        "  decoder_output = decoder(bottleneck_output)\n",
        "\n",
        "  model = tf.keras.Model(inputs =inputs, outputs=decoder_output)\n",
        "  encoder_model = tf.keras.Model(inputs=inputs, outputs=encoder_visualization)\n",
        "  return model, encoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1MmS7r0tkuIf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 7, 7, 128)         295040    \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2D  (None, 14, 14, 128)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 14, 14, 64)        73792     \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSampling  (None, 28, 28, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 1)         577       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 739,073\n",
            "Trainable params: 739,073\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "convolutional_model, convolutional_encoder_model = convolutional_auto_encoder()\n",
        "convolutional_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCh0ea08lERp"
      },
      "source": [
        "## Compile and Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0Umj_xaiHL_"
      },
      "outputs": [],
      "source": [
        "train_steps = 60000 // BATCH_SIZE\n",
        "valid_steps = 60000 // BATCH_SIZE\n",
        "\n",
        "convolutional_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy')\n",
        "conv_model_history = convolutional_model.fit(train_dataset, steps_per_epoch=train_steps, validation_data=test_dataset, validation_steps=valid_steps, epochs=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npl9MHErlJa2"
      },
      "source": [
        "## Display sample results\n",
        "\n",
        "Let's see if the model can generate the clean image from noisy inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqFR12I6fwBe"
      },
      "outputs": [],
      "source": [
        "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
        "  '''Display sample outputs in one row.'''\n",
        "  for idx, noisy_image in enumerate(disp_images):\n",
        "    plt.subplot(3, 10, offset + idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    noisy_image = np.reshape(noisy_image, shape)\n",
        "    plt.imshow(noisy_image, cmap='gray')\n",
        "\n",
        "\n",
        "def display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape=(8,4)):\n",
        "  '''Displays the input, encoded, and decoded output values.'''\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  display_one_row(disp_input_images, 0, shape=(28,28,))\n",
        "  display_one_row(disp_encoded, 10, shape=enc_shape)\n",
        "  display_one_row(disp_predicted, 20, shape=(28,28,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtQyQRxRN_hH"
      },
      "outputs": [],
      "source": [
        "# take 1 batch of the dataset\n",
        "test_dataset = test_dataset.take(1)\n",
        "\n",
        "# take the input images and put them in a list\n",
        "output_samples = []\n",
        "for input_image, image in tfds.as_numpy(test_dataset):\n",
        "      output_samples = input_image\n",
        "\n",
        "# pick 10 indices\n",
        "idxs = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "# prepare test samples as a batch of 10 images\n",
        "conv_output_samples = np.array(output_samples[idxs])\n",
        "conv_output_samples = np.reshape(conv_output_samples, (10, 28, 28, 1))\n",
        "\n",
        "# get the encoder ouput\n",
        "encoded = convolutional_encoder_model.predict(conv_output_samples)\n",
        "\n",
        "# get a prediction for some values in the dataset\n",
        "predicted = convolutional_model.predict(conv_output_samples)\n",
        "\n",
        "# display the samples, encodings and decoded values!\n",
        "display_results(conv_output_samples, encoded, predicted, enc_shape=(7,7))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
